Based on Hugging Face code found here:

https://github.com/huggingface/transformers


Set up AWS p2 instance.  Connect via ssh with .pem file:

ssh -i aws_gputest_1.pem ubuntu@3.17.70.161

source activate pytorch_p36


Sanity check:  usually not needed, but can test if PyTorch is using GPU:

python3

import torch
torch.cuda.current_device()
torch.cuda.device(0)
torch.cuda.device_count()
torch.cuda.get_device_name(0)
torch.cuda.is_available()


pip install --upgrade pip

pip install transformers

git clone https://github.com/huggingface/transformers.git

git clone https://github.com/jasonrohrer/jason-rohrer.git

cd jason-rohrer/ai/text/transformersMod

python ./run_generation.py --model_type=gpt2 --length=20 --model_name_or_path=gpt2

python ./run_generation.py --model_type=gpt2 --length=200 --model_name_or_path=gpt2-xl


python ./run_generation.py --model_type=gpt2 --length=20 --model_name_or_path=gpt2-xl --out_file=out.txt --in_file=sampleInputShort.txt --gen_words=500




After initial run to warm up caches, running this test:

time python ./run_generation.py --model_type=gpt2 --length=20 --model_name_or_path=gpt2-xl --out_file=out.txt --in_file=sampleInputShort.txt --gen_words=500


p3 instance completes it in:
4m43.613s

p2 instance completes it in:
22m16.647s


p3 is almost 5x faster.  Really worth the cost!




<|endoftext|> token number:
50256


<|endoftext|>Trump on what's next after AG Jeff Sessions:


Actually, this seems to help to prevent <|endoftext|> from being generated:

--stop_token="<|endoftext|>"

This says "end each generation before <|endoftext|>, which means that if we keep
going, generating more, then we trim each segment to not include <|endoftext|>
and try again from there to generate more.





Noticing that p3 and p2 output differ for short text seed on seed 42.
(With pre-reseed code, commit 5472c5becf6f82b1401bfe90debbd05e2c6cd21d )


"hunting a pig" not present in p3 output


Added printing of cumu text along the way.

Testing on both with:

python ./run_generation.py --model_type=gpt2 --length=20 --model_name_or_path=gpt2-xl --out_file=outP2P3.txt --in_file=sampleInputShort.txt --gen_words=5000 > runOut.txt
